{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepSpeech v0.7.0",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saumya0303/Deepspeech_Colab/blob/master/DeepSpeech_v0_7_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2IFaKpPpUnf",
        "colab_type": "code",
        "outputId": "7e27e0c9-f341-45cc-c8b0-3a1574916661",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n",
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc0U1B07pqCr",
        "colab_type": "code",
        "outputId": "8596115c-3c8f-46b4-d346-6590a5052121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "#git lfs install\n",
        "!sudo apt-get install git-lfs\n",
        "!git lfs version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 0s (15.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "git-lfs/2.3.4 (GitHub; linux amd64; go 1.8.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXeaTqz_qA3j",
        "colab_type": "code",
        "outputId": "c6cb32be-e706-40b9-8a56-86d3344796b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#cloning the DeepSpeech in the virtual space alloted by colab and installing the requirements\n",
        "!git lfs clone -b v0.7.0 https://github.com/mozilla/DeepSpeech\n",
        "%cd '/content/DeepSpeech/'\n",
        "!pip3 install --upgrade pip==20.0.2 wheel==0.34.2 setuptools==46.1.3\n",
        "!pip3 install --upgrade --force-reinstall -e .\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
            "          with new flags from 'git clone'\n",
            "\n",
            "'git clone' has been updated in upstream Git to have comparable\n",
            "speeds to 'git lfs clone'.\n",
            "Cloning into 'DeepSpeech'...\n",
            "remote: Enumerating objects: 132, done.\u001b[K\n",
            "remote: Counting objects: 100% (132/132), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 18114 (delta 78), reused 85 (delta 62), pack-reused 17982\u001b[K\n",
            "Receiving objects: 100% (18114/18114), 47.60 MiB | 21.88 MiB/s, done.\n",
            "Resolving deltas: 100% (12299/12299), done.\n",
            "Note: checking out '3fbbca2b55e73adecf9a8b93b7ea82b8a08e640e'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "Git LFS: (1 of 1 files) 909.20 MB / 909.20 MB\n",
            "/content/DeepSpeech\n",
            "Collecting pip==20.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 9.2MB/s \n",
            "\u001b[?25hRequirement already up-to-date: wheel==0.34.2 in /usr/local/lib/python3.6/dist-packages (0.34.2)\n",
            "Requirement already up-to-date: setuptools==46.1.3 in /usr/local/lib/python3.6/dist-packages (46.1.3)\n",
            "Installing collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-20.0.2\n",
            "Obtaining file:///content/DeepSpeech\n",
            "Collecting tensorflow==1.15.2\n",
            "  Downloading tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 31 kB/s \n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading numpy-1.18.3-cp36-cp36m-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting progressbar2\n",
            "  Downloading progressbar2-3.51.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 30 kB/s \n",
            "\u001b[?25hCollecting six\n",
            "  Downloading six-1.14.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pyxdg\n",
            "  Downloading pyxdg-0.26-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting attrdict\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Collecting absl-py\n",
            "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 78.6 MB/s \n",
            "\u001b[?25hCollecting semver\n",
            "  Downloading semver-2.9.1-py2.py3-none-any.whl (9.8 kB)\n",
            "Collecting opuslib==2.0.0\n",
            "  Downloading opuslib-2.0.0.tar.gz (7.3 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-1.3.0.tar.gz (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 73.0 MB/s \n",
            "\u001b[?25hCollecting sox\n",
            "  Downloading sox-1.3.7-py2.py3-none-any.whl (34 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.0.3-cp36-cp36m-manylinux1_x86_64.whl (10.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.0 MB 79.8 MB/s \n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting numba==0.47.0\n",
            "  Downloading numba-0.47.0-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 71.6 MB/s \n",
            "\u001b[?25hCollecting llvmlite==0.31.0\n",
            "  Downloading llvmlite-0.31.0-cp36-cp36m-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2 MB 118.5 MB/s \n",
            "\u001b[?25hCollecting librosa\n",
            "  Downloading librosa-0.7.2.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 75.7 MB/s \n",
            "\u001b[?25hCollecting soundfile\n",
            "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
            "Collecting ds_ctcdecoder@ https://community-tc.services.mozilla.com/api/index/v1/task/project.deepspeech.deepspeech.native_client.v0.7.0.cpu-ctc/artifacts/public/ds_ctcdecoder-0.7.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "  Downloading https://community-tc.services.mozilla.com/api/index/v1/task/project.deepspeech.deepspeech.native_client.v0.7.0.cpu-ctc/artifacts/public/ds_ctcdecoder-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 11.2 MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting termcolor>=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 61.0 MB/s \n",
            "\u001b[?25hCollecting astor>=0.6.0\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting google-pasta>=0.1.6\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 54.9 MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.6.1\n",
            "  Downloading protobuf-3.11.3-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 71.5 MB/s \n",
            "\u001b[?25hCollecting wheel>=0.26; python_version >= \"3\"\n",
            "  Downloading wheel-0.34.2-py2.py3-none-any.whl (26 kB)\n",
            "Collecting keras-preprocessing>=1.0.5\n",
            "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 706 kB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting grpcio>=1.8.6\n",
            "  Downloading grpcio-1.28.1-cp36-cp36m-manylinux2010_x86_64.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 59.6 MB/s \n",
            "\u001b[?25hCollecting python-utils>=2.3.0\n",
            "  Downloading python_utils-2.4.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.4.2.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 62.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.1.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 10.4 MB/s \n",
            "\u001b[?25hCollecting cmaes\n",
            "  Downloading cmaes-0.5.0-py3-none-any.whl (13 kB)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-4.1.0-py2.py3-none-any.whl (14 kB)\n",
            "Collecting joblib\n",
            "  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 69.0 MB/s \n",
            "\u001b[?25hCollecting scipy!=1.4.0\n",
            "  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.1 MB 19 kB/s \n",
            "\u001b[?25hCollecting sqlalchemy>=1.1.0\n",
            "  Downloading SQLAlchemy-1.3.16-cp36-cp36m-manylinux2010_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 59.0 MB/s \n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.45.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.9.0-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 87.5 MB/s \n",
            "\u001b[?25hCollecting python-dateutil>=2.6.1\n",
            "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 56.5 MB/s \n",
            "\u001b[?25hCollecting pytz>=2017.2\n",
            "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 66.2 MB/s \n",
            "\u001b[?25hCollecting idna<3,>=2.5\n",
            "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting chardet<4,>=3.0.2\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 85.0 MB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2020.4.5.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 76.6 MB/s \n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 85.8 MB/s \n",
            "\u001b[?25hCollecting setuptools\n",
            "  Using cached setuptools-46.1.3-py3-none-any.whl (582 kB)\n",
            "Collecting audioread>=2.0.0\n",
            "  Downloading audioread-2.1.8.tar.gz (21 kB)\n",
            "Collecting scikit-learn!=0.19.0,>=0.14.0\n",
            "  Downloading scikit_learn-0.22.2.post1-cp36-cp36m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 68.2 MB/s \n",
            "\u001b[?25hCollecting decorator>=3.0.0\n",
            "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
            "Collecting resampy>=0.2.2\n",
            "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
            "\u001b[K     |████████████████████████████████| 323 kB 61.3 MB/s \n",
            "\u001b[?25hCollecting cffi>=1.0\n",
            "  Downloading cffi-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (399 kB)\n",
            "\u001b[K     |████████████████████████████████| 399 kB 57.1 MB/s \n",
            "\u001b[?25hCollecting h5py\n",
            "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 68.3 MB/s \n",
            "\u001b[?25hCollecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 70.7 MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 9.3 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.2-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting PrettyTable<0.8,>=0.7.2\n",
            "  Downloading prettytable-0.7.2.tar.bz2 (21 kB)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.4.5-py2.py3-none-any.whl (110 kB)\n",
            "\u001b[K     |████████████████████████████████| 110 kB 65.4 MB/s \n",
            "\u001b[?25hCollecting stevedore>=1.20.0\n",
            "  Downloading stevedore-1.32.0-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=3.12\n",
            "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 75.0 MB/s \n",
            "\u001b[?25hCollecting pyparsing>=2.1.0\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting cmd2!=0.8.3,<0.9.0,>=0.8.0\n",
            "  Downloading cmd2-0.8.9-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.0-py2.py3-none-any.whl (32 kB)\n",
            "Collecting pycparser\n",
            "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 77.2 MB/s \n",
            "\u001b[?25hCollecting MarkupSafe>=0.9.2\n",
            "  Downloading MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (27 kB)\n",
            "Collecting wcwidth; sys_platform != \"win32\"\n",
            "  Downloading wcwidth-0.1.9-py2.py3-none-any.whl (19 kB)\n",
            "Collecting pyperclip\n",
            "  Downloading pyperclip-1.8.0.tar.gz (16 kB)\n",
            "Building wheels for collected packages: absl-py, opuslib, optuna, bs4, librosa, termcolor, wrapt, gast, alembic, audioread, resampy, PrettyTable, PyYAML, pyperclip\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=2adb1ef48e3150426c4e895b3ac5f0991b859471b575d1a334ab7b3a157ea9ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
            "  Building wheel for opuslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opuslib: filename=opuslib-2.0.0-py3-none-any.whl size=11009 sha256=e2f2a63f821adbb39c88e0ce00fcf3ee8facc349f7649f865f3e55cd142fac0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/01/88/37797e9e9d157a33eefed22a46aa0bf5044effcec6a9181e41\n",
            "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-1.3.0-py3-none-any.whl size=221120 sha256=bb9b7c90cf6e03e31310174dad8fb111fff41ec075221c9d28aae6036c4c11ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/f9/1c/649ef739b35d9fc6ba0752053b997e3aa1bc30e5662f295f18\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=744ef307926bbb140d24b97fea36ded97f20a75d8f6a32ad512839fca5b499b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/6d/a97dd4f22376d4472d5f4c76c7646876052ff3166b3cf71050\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.7.2-py3-none-any.whl size=1612883 sha256=e30c67da7ec745e7990efb9b3dc854e7f33b9eddad9f96a65a7e32028fc22bf4\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/1d/15/a479fa740849128d481333d2f354f97691be3e2c82480a3e00\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=6591cf17ab3f3d738f031be94865df4879fffe069640aef8485ccd4b2e9a5375\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=67507 sha256=c00a8825ae0de506d201eb0af118626f923906f2d907dfe4ae35d1760c2241e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=5c5dde243f4522d00a21737b7ff6604abd49318b0e5864544d179c8e009ab096\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n",
            "  Building wheel for alembic (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.2-py2.py3-none-any.whl size=159543 sha256=a3959a31b6dc68ef8f3366a647eed8c46f14b3ebbf54ef1b74e657ae4717429b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/50/61/5cc491b0ca39be60dfb4dce940b389ff91b847d62e0eb2d680\n",
            "  Building wheel for audioread (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for audioread: filename=audioread-2.1.8-py3-none-any.whl size=23091 sha256=2f4a4450790fc3e2891c90b95fde60658474f04a999af85ee3ba22e61e6a16d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/9a/c7/249a5daf5cb90d9786afaec371cba9dc43f04f916db5d1caff\n",
            "  Building wheel for resampy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320720 sha256=686c4d609b8ea94d747f6d8555c862d600f91a421941e4ec697c01979a30c3fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/d4/04/49d8824a42bd9f9b11d502727965b9997f0d41d2b22ae4f645\n",
            "  Building wheel for PrettyTable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PrettyTable: filename=prettytable-0.7.2-py3-none-any.whl size=13698 sha256=48e44cf0559f20f08aff10d6eaa2aee0b9421f04ba9a2f6dc44c9834dcfec439\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/15/c3/5f28b42ae9c81638570b8b7ed654e0f98c5fdc08875869511b\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=e87c5e56d15ea95f04ec36b334df027c0cbce234b901afc8b7a05d7b71018c5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/9d/ad/2ee53cf262cba1ffd8afe1487eef788ea3f260b7e6232a80fc\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.0-py3-none-any.whl size=8691 sha256=f43ad72687c7c795d04b55dfabb52c9aeb10f3fe85ffcecff401a63d1fb505aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/30/fe/92e2d4b1301ba74c07ea09c9e4c08f5bf12bae9c30319d74c5\n",
            "Successfully built absl-py opuslib optuna bs4 librosa termcolor wrapt gast alembic audioread resampy PrettyTable PyYAML pyperclip\n",
            "\u001b[31mERROR: umap-learn 0.4.1 has requirement numba!=0.47,>=0.46, but you'll have numba 0.47.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0rc0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: convertdate 2.2.0 has requirement pytz<2020,>=2014.10, but you'll have pytz 2020.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, six, h5py, keras-applications, termcolor, opt-einsum, tensorflow-estimator, astor, google-pasta, wrapt, setuptools, protobuf, absl-py, werkzeug, grpcio, wheel, markdown, tensorboard, keras-preprocessing, gast, tensorflow, python-utils, progressbar2, pyxdg, attrdict, semver, opuslib, python-editor, python-dateutil, sqlalchemy, MarkupSafe, Mako, alembic, PrettyTable, pbr, stevedore, PyYAML, pyparsing, wcwidth, pyperclip, cmd2, cliff, cmaes, colorlog, joblib, scipy, tqdm, optuna, sox, soupsieve, beautifulsoup4, bs4, pytz, pandas, idna, chardet, certifi, urllib3, requests, llvmlite, numba, audioread, scikit-learn, decorator, resampy, pycparser, cffi, soundfile, librosa, ds-ctcdecoder, deepspeech-training\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.18.3\n",
            "    Uninstalling numpy-1.18.3:\n",
            "      Successfully uninstalled numpy-1.18.3\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Attempting uninstall: keras-applications\n",
            "    Found existing installation: Keras-Applications 1.0.8\n",
            "    Uninstalling Keras-Applications-1.0.8:\n",
            "      Successfully uninstalled Keras-Applications-1.0.8\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 1.1.0\n",
            "    Uninstalling termcolor-1.1.0:\n",
            "      Successfully uninstalled termcolor-1.1.0\n",
            "  Attempting uninstall: opt-einsum\n",
            "    Found existing installation: opt-einsum 3.2.1\n",
            "    Uninstalling opt-einsum-3.2.1:\n",
            "      Successfully uninstalled opt-einsum-3.2.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Attempting uninstall: astor\n",
            "    Found existing installation: astor 0.8.1\n",
            "    Uninstalling astor-0.8.1:\n",
            "      Successfully uninstalled astor-0.8.1\n",
            "  Attempting uninstall: google-pasta\n",
            "    Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 46.1.3\n",
            "    Uninstalling setuptools-46.1.3:\n",
            "      Successfully uninstalled setuptools-46.1.3\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.10.0\n",
            "    Uninstalling protobuf-3.10.0:\n",
            "      Successfully uninstalled protobuf-3.10.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 0.9.0\n",
            "    Uninstalling absl-py-0.9.0:\n",
            "      Successfully uninstalled absl-py-0.9.0\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.28.1\n",
            "    Uninstalling grpcio-1.28.1:\n",
            "      Successfully uninstalled grpcio-1.28.1\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.34.2\n",
            "    Uninstalling wheel-0.34.2:\n",
            "      Successfully uninstalled wheel-0.34.2\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.2.1\n",
            "    Uninstalling Markdown-3.2.1:\n",
            "      Successfully uninstalled Markdown-3.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.2.1\n",
            "    Uninstalling tensorboard-2.2.1:\n",
            "      Successfully uninstalled tensorboard-2.2.1\n",
            "  Attempting uninstall: keras-preprocessing\n",
            "    Found existing installation: Keras-Preprocessing 1.1.0\n",
            "    Uninstalling Keras-Preprocessing-1.1.0:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.2.0rc3\n",
            "    Uninstalling tensorflow-2.2.0rc3:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc3\n",
            "  Attempting uninstall: python-utils\n",
            "    Found existing installation: python-utils 2.4.0\n",
            "    Uninstalling python-utils-2.4.0:\n",
            "      Successfully uninstalled python-utils-2.4.0\n",
            "  Attempting uninstall: progressbar2\n",
            "    Found existing installation: progressbar2 3.38.0\n",
            "    Uninstalling progressbar2-3.38.0:\n",
            "      Successfully uninstalled progressbar2-3.38.0\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.1\n",
            "    Uninstalling python-dateutil-2.8.1:\n",
            "      Successfully uninstalled python-dateutil-2.8.1\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 1.3.16\n",
            "    Uninstalling SQLAlchemy-1.3.16:\n",
            "      Successfully uninstalled SQLAlchemy-1.3.16\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 1.1.1\n",
            "    Uninstalling MarkupSafe-1.1.1:\n",
            "      Successfully uninstalled MarkupSafe-1.1.1\n",
            "  Attempting uninstall: PrettyTable\n",
            "    Found existing installation: prettytable 0.7.2\n",
            "    Uninstalling prettytable-0.7.2:\n",
            "      Successfully uninstalled prettytable-0.7.2\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 2.4.7\n",
            "    Uninstalling pyparsing-2.4.7:\n",
            "      Successfully uninstalled pyparsing-2.4.7\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.1.9\n",
            "    Uninstalling wcwidth-0.1.9:\n",
            "      Successfully uninstalled wcwidth-0.1.9\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 0.14.1\n",
            "    Uninstalling joblib-0.14.1:\n",
            "      Successfully uninstalled joblib-0.14.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.38.0\n",
            "    Uninstalling tqdm-4.38.0:\n",
            "      Successfully uninstalled tqdm-4.38.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Attempting uninstall: bs4\n",
            "    Found existing installation: bs4 0.0.1\n",
            "    Uninstalling bs4-0.0.1:\n",
            "      Successfully uninstalled bs4-0.0.1\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.0.3\n",
            "    Uninstalling pandas-1.0.3:\n",
            "      Successfully uninstalled pandas-1.0.3\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.8\n",
            "    Uninstalling idna-2.8:\n",
            "      Successfully uninstalled idna-2.8\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2020.4.5.1\n",
            "    Uninstalling certifi-2020.4.5.1:\n",
            "      Successfully uninstalled certifi-2020.4.5.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.21.0\n",
            "    Uninstalling requests-2.21.0:\n",
            "      Successfully uninstalled requests-2.21.0\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.31.0\n",
            "    Uninstalling llvmlite-0.31.0:\n",
            "      Successfully uninstalled llvmlite-0.31.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.48.0\n",
            "    Uninstalling numba-0.48.0:\n",
            "      Successfully uninstalled numba-0.48.0\n",
            "  Attempting uninstall: audioread\n",
            "    Found existing installation: audioread 2.1.8\n",
            "    Uninstalling audioread-2.1.8:\n",
            "      Successfully uninstalled audioread-2.1.8\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: resampy\n",
            "    Found existing installation: resampy 0.2.2\n",
            "    Uninstalling resampy-0.2.2:\n",
            "      Successfully uninstalled resampy-0.2.2\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.20\n",
            "    Uninstalling pycparser-2.20:\n",
            "      Successfully uninstalled pycparser-2.20\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.14.0\n",
            "    Uninstalling cffi-1.14.0:\n",
            "      Successfully uninstalled cffi-1.14.0\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.6.3\n",
            "    Uninstalling librosa-0.6.3:\n",
            "      Successfully uninstalled librosa-0.6.3\n",
            "  Running setup.py develop for deepspeech-training\n",
            "Successfully installed Mako-1.1.2 MarkupSafe-1.1.1 PrettyTable-0.7.2 PyYAML-5.3.1 absl-py-0.9.0 alembic-1.4.2 astor-0.8.1 attrdict-2.0.1 audioread-2.1.8 beautifulsoup4-4.9.0 bs4-0.0.1 certifi-2020.4.5.1 cffi-1.14.0 chardet-3.0.4 cliff-3.1.0 cmaes-0.5.0 cmd2-0.8.9 colorlog-4.1.0 decorator-4.4.2 deepspeech-training ds-ctcdecoder-0.7.0 gast-0.2.2 google-pasta-0.2.0 grpcio-1.28.1 h5py-2.10.0 idna-2.9 joblib-0.14.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 librosa-0.7.2 llvmlite-0.31.0 markdown-3.2.1 numba-0.47.0 numpy-1.18.3 opt-einsum-3.2.1 optuna-1.3.0 opuslib-2.0.0 pandas-1.0.3 pbr-5.4.5 progressbar2-3.51.1 protobuf-3.11.3 pycparser-2.20 pyparsing-2.4.7 pyperclip-1.8.0 python-dateutil-2.8.1 python-editor-1.0.4 python-utils-2.4.0 pytz-2020.1 pyxdg-0.26 requests-2.23.0 resampy-0.2.2 scikit-learn-0.22.2.post1 scipy-1.4.1 semver-2.9.1 setuptools-46.1.3 six-1.14.0 soundfile-0.10.3.post1 soupsieve-2.0 sox-1.3.7 sqlalchemy-1.3.16 stevedore-1.32.0 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1 termcolor-1.1.0 tqdm-4.45.0 urllib3-1.25.9 wcwidth-0.1.9 werkzeug-1.0.1 wheel-0.34.2 wrapt-1.12.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "cffi",
                  "chardet",
                  "dateutil",
                  "decorator",
                  "google",
                  "grpc",
                  "idna",
                  "numpy",
                  "pandas",
                  "pkg_resources",
                  "pyparsing",
                  "pytz",
                  "requests",
                  "six",
                  "tqdm",
                  "urllib3",
                  "wcwidth"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Sn0W5VGdZHZ",
        "colab_type": "code",
        "outputId": "4b1d4b67-8f0b-4df3-ad18-0fe17307c7b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "#getting the required tensorflow version\n",
        "!pip3 uninstall tensorflow\n",
        "!pip3 install 'tensorflow-gpu==1.15.2'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow 1.15.2\n",
            "Uninstalling tensorflow-1.15.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/freeze_graph\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-1.15.2.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow_core/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-1.15.2\n",
            "Collecting tensorflow-gpu==1.15.2\n",
            "  Downloading tensorflow_gpu-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (411.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 411.0 MB 35 kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (0.9.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.28.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (3.11.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (3.2.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.18.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.14.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15.2) (46.1.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.2.1)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHDBC5uBrTBr",
        "colab_type": "code",
        "outputId": "d1eea4c6-2ea2-4680-a457-9361cb81b8e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "#downloading the V0.7.0 checkpoint\n",
        "%cd '/content/drive/My Drive/transfer_learning/'\n",
        "!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.7.0/deepspeech-0.7.0-checkpoint.tar.gz\n",
        "!tar xvfz deepspeech-0.7.0-checkpoint.tar.gz\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/transfer_learning\n",
            "--2020-04-30 12:27:10--  https://github.com/mozilla/DeepSpeech/releases/download/v0.7.0/deepspeech-0.7.0-checkpoint.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.118.3\n",
            "Connecting to github.com (github.com)|140.82.118.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/60273704/640fb500-856a-11ea-9f43-2230f435681d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200430%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200430T122710Z&X-Amz-Expires=300&X-Amz-Signature=46bd088c715a1b5576cf090cb51c1b2e38903eb627a8a3fff3610085da339b31&X-Amz-SignedHeaders=host&actor_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Ddeepspeech-0.7.0-checkpoint.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-04-30 12:27:10--  https://github-production-release-asset-2e65be.s3.amazonaws.com/60273704/640fb500-856a-11ea-9f43-2230f435681d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200430%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200430T122710Z&X-Amz-Expires=300&X-Amz-Signature=46bd088c715a1b5576cf090cb51c1b2e38903eb627a8a3fff3610085da339b31&X-Amz-SignedHeaders=host&actor_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Ddeepspeech-0.7.0-checkpoint.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.8.83\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.8.83|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 644569969 (615M) [application/octet-stream]\n",
            "Saving to: ‘deepspeech-0.7.0-checkpoint.tar.gz’\n",
            "\n",
            "deepspeech-0.7.0-ch 100%[===================>] 614.71M  34.5MB/s    in 20s     \n",
            "\n",
            "2020-04-30 12:27:31 (30.9 MB/s) - ‘deepspeech-0.7.0-checkpoint.tar.gz’ saved [644569969/644569969]\n",
            "\n",
            "deepspeech-0.7.0-checkpoint/\n",
            "deepspeech-0.7.0-checkpoint/checkpoint\n",
            "deepspeech-0.7.0-checkpoint/best_dev-732522.data-00000-of-00001\n",
            "deepspeech-0.7.0-checkpoint/best_dev-732522.index\n",
            "deepspeech-0.7.0-checkpoint/best_dev_checkpoint\n",
            "deepspeech-0.7.0-checkpoint/best_dev-732522.meta\n",
            "deepspeech-0.7.0-checkpoint/alphabet.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upvdr4X3NZmm",
        "colab_type": "code",
        "outputId": "673a6964-5dd3-41ba-8497-ea91966ba4b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#testing if the downloaded code is working\n",
        "%cd '/content/DeepSpeech/'\n",
        "!python '/content/DeepSpeech/DeepSpeech.py' --helpfull\n",
        "! '/content/DeepSpeech/bin/run-ldc93s1.sh'\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DeepSpeech\n",
            "\n",
            "       USAGE: /content/DeepSpeech/DeepSpeech.py [flags]\n",
            "flags:\n",
            "\n",
            "absl.app:\n",
            "  -?,--[no]help: show this help\n",
            "    (default: 'false')\n",
            "  --[no]helpfull: show full help\n",
            "    (default: 'false')\n",
            "  --[no]helpshort: show this help\n",
            "    (default: 'false')\n",
            "  --[no]helpxml: like --helpfull, but generates XML output\n",
            "    (default: 'false')\n",
            "  --[no]only_check_args: Set to true to validate args and exit.\n",
            "    (default: 'false')\n",
            "  --[no]pdb_post_mortem: Set to true to handle uncaught exceptions with PDB post\n",
            "    mortem.\n",
            "    (default: 'false')\n",
            "  --profile_file: Dump profile information to a file (for python -m pstats).\n",
            "    Implies --run_with_profiling.\n",
            "  --[no]run_with_pdb: Set to true for PDB debug mode\n",
            "    (default: 'false')\n",
            "  --[no]run_with_profiling: Set to true for profiling the script. Execution will\n",
            "    be slower, and the output format might change over time.\n",
            "    (default: 'false')\n",
            "  --[no]use_cprofile_for_profiling: Use cProfile instead of the profile module\n",
            "    for profiling. This has no effect unless --run_with_profiling is set.\n",
            "    (default: 'true')\n",
            "\n",
            "absl.logging:\n",
            "  --[no]alsologtostderr: also log to stderr?\n",
            "    (default: 'false')\n",
            "  --log_dir: directory to write logfiles into\n",
            "    (default: '')\n",
            "  --[no]logtostderr: Should only log to stderr?\n",
            "    (default: 'false')\n",
            "  --[no]showprefixforinfo: If False, do not prepend prefix to info messages when\n",
            "    it's logged to stderr, --verbosity is set to INFO level, and python logging\n",
            "    is used.\n",
            "    (default: 'true')\n",
            "  --stderrthreshold: log messages at this level, or more severe, to stderr in\n",
            "    addition to the logfile.  Possible values are 'debug', 'info', 'warning',\n",
            "    'error', and 'fatal'.  Obsoletes --alsologtostderr. Using --alsologtostderr\n",
            "    cancels the effect of this flag. Please also note that this flag is subject\n",
            "    to --verbosity and requires logfile not be stderr.\n",
            "    (default: 'fatal')\n",
            "  -v,--verbosity: Logging verbosity level. Messages logged at this level or\n",
            "    lower will be included. Set to 1 for debug logging. If the flag was not set\n",
            "    or supplied, the value will be changed from the default of -1 (warning) to 0\n",
            "    (info) after flags are parsed.\n",
            "    (default: '-1')\n",
            "    (an integer)\n",
            "\n",
            "absl.testing.absltest:\n",
            "  --test_random_seed: Random seed for testing. Some test frameworks may change\n",
            "    the default value of this flag between runs, so it is not appropriate for\n",
            "    seeding probabilistic tests.\n",
            "    (default: '301')\n",
            "    (an integer)\n",
            "  --test_randomize_ordering_seed: If positive, use this as a seed to randomize\n",
            "    the execution order for test cases. If \"random\", pick a random seed to use.\n",
            "    If 0 or not set, do not randomize test case execution order. This flag also\n",
            "    overrides the TEST_RANDOMIZE_ORDERING_SEED environment variable.\n",
            "    (default: '')\n",
            "  --test_srcdir: Root of directory tree where source files live\n",
            "    (default: '')\n",
            "  --test_tmpdir: Directory for temporary testing files\n",
            "    (default: '/tmp/absl_testing')\n",
            "  --xml_output_file: File to store XML test results\n",
            "    (default: '')\n",
            "\n",
            "deepspeech_training.util.flags:\n",
            "  --alphabet_config_path: path to the configuration file specifying the alphabet\n",
            "    used by the network. See the comment in data/alphabet.txt for a description\n",
            "    of the format.\n",
            "    (default: 'data/alphabet.txt')\n",
            "  --audio_sample_rate: sample rate value expected by model\n",
            "    (default: '16000')\n",
            "    (an integer)\n",
            "  --[no]augmentation_freq_and_time_masking: whether to use frequency and time\n",
            "    masking augmentation\n",
            "    (default: 'false')\n",
            "  --augmentation_freq_and_time_masking_freq_mask_range: max range of masks in\n",
            "    the frequency domain when performing freqtime-mask augmentation\n",
            "    (default: '5')\n",
            "    (an integer)\n",
            "  --augmentation_freq_and_time_masking_number_freq_masks: number of masks in the\n",
            "    frequency domain when performing freqtime-mask augmentation\n",
            "    (default: '3')\n",
            "    (an integer)\n",
            "  --augmentation_freq_and_time_masking_number_time_masks: number of masks in the\n",
            "    time domain when performing freqtime-mask augmentation\n",
            "    (default: '3')\n",
            "    (an integer)\n",
            "  --augmentation_freq_and_time_masking_time_mask_range: max range of masks in\n",
            "    the time domain when performing freqtime-mask augmentation\n",
            "    (default: '2')\n",
            "    (an integer)\n",
            "  --[no]augmentation_pitch_and_tempo_scaling: whether to use spectrogram speed\n",
            "    and tempo scaling\n",
            "    (default: 'false')\n",
            "  --augmentation_pitch_and_tempo_scaling_max_pitch: max value of pitch scaling\n",
            "    (default: '1.2')\n",
            "    (a number)\n",
            "  --augmentation_pitch_and_tempo_scaling_max_tempo: max vlaue of tempo scaling\n",
            "    (default: '1.2')\n",
            "    (a number)\n",
            "  --augmentation_pitch_and_tempo_scaling_min_pitch: min value of pitch scaling\n",
            "    (default: '0.95')\n",
            "    (a number)\n",
            "  --[no]augmentation_sparse_warp: whether to use spectrogram sparse warp. USE OF\n",
            "    THIS FLAG IS UNSUPPORTED, enable sparse warp will increase training time\n",
            "    drastically, and the paper also mentioned that this is not a major factor to\n",
            "    improve accuracy.\n",
            "    (default: 'false')\n",
            "  --augmentation_sparse_warp_interpolation_order:\n",
            "    sparse_warp_interpolation_order\n",
            "    (default: '2')\n",
            "    (an integer)\n",
            "  --augmentation_sparse_warp_num_boundary_points:\n",
            "    sparse_warp_num_boundary_points\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --augmentation_sparse_warp_num_control_points: specify number of control\n",
            "    points\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --augmentation_sparse_warp_regularization_weight:\n",
            "    sparse_warp_regularization_weight\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --augmentation_sparse_warp_time_warping_para: time_warping_para\n",
            "    (default: '20')\n",
            "    (an integer)\n",
            "  --augmentation_spec_dropout_keeprate: keep rate of dropout augmentation on\n",
            "    spectrogram (if 1, no dropout will be performed on spectrogram)\n",
            "    (default: '1.0')\n",
            "    (a number)\n",
            "  --augmentation_speed_up_std: std for speeding-up tempo. If std is 0, this\n",
            "    augmentation is not performed\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --[no]automatic_mixed_precision: whether to allow automatic mixed precision\n",
            "    training. USE OF THIS FLAG IS UNSUPPORTED. Checkpoints created with\n",
            "    automatic mixed precision training will not be usable without mixed\n",
            "    precision.\n",
            "    (default: 'false')\n",
            "  --beam_width: beam width used in the CTC decoder when building candidate\n",
            "    transcriptions\n",
            "    (default: '1024')\n",
            "    (an integer)\n",
            "  --beta1: beta 1 parameter of Adam optimizer\n",
            "    (default: '0.9')\n",
            "    (a number)\n",
            "  --beta2: beta 2 parameter of Adam optimizer\n",
            "    (default: '0.999')\n",
            "    (a number)\n",
            "  --checkpoint_dir: directory from which checkpoints are loaded and to which\n",
            "    they are saved - defaults to directory \"deepspeech/checkpoints\" within\n",
            "    user's data home specified by the XDG Base Directory Specification\n",
            "    (default: '')\n",
            "  --checkpoint_secs: checkpoint saving interval in seconds\n",
            "    (default: '600')\n",
            "    (an integer)\n",
            "  --cutoff_prob: only consider characters until this probability mass is\n",
            "    reached. 1.0 = disabled.\n",
            "    (default: '1.0')\n",
            "    (a number)\n",
            "  --cutoff_top_n: only process this number of characters sorted by probability\n",
            "    mass for each time step. If bigger than alphabet size, disabled.\n",
            "    (default: '300')\n",
            "    (an integer)\n",
            "  --data_aug_features_additive: std of the Gaussian additive noise\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --data_aug_features_multiplicative: std of normal distribution around 1 for\n",
            "    multiplicative noise\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --dev_batch_size: number of elements in a validation batch\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --dev_files: comma separated list of files specifying the dataset used for\n",
            "    validation. Multiple files will get merged. If empty, validation will not be\n",
            "    run.\n",
            "    (default: '')\n",
            "  --drop_source_layers: single integer for how many layers to drop from source\n",
            "    model (to drop just output == 1, drop penultimate and output ==2, etc)\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --dropout_rate: dropout rate for feedforward layers\n",
            "    (default: '0.05')\n",
            "    (a number)\n",
            "  --dropout_rate2: dropout rate for layer 2 - defaults to dropout_rate\n",
            "    (default: '-1.0')\n",
            "    (a number)\n",
            "  --dropout_rate3: dropout rate for layer 3 - defaults to dropout_rate\n",
            "    (default: '-1.0')\n",
            "    (a number)\n",
            "  --dropout_rate4: dropout rate for layer 4 - defaults to 0.0\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --dropout_rate5: dropout rate for layer 5 - defaults to 0.0\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --dropout_rate6: dropout rate for layer 6 - defaults to dropout_rate\n",
            "    (default: '-1.0')\n",
            "    (a number)\n",
            "  --[no]early_stop: Enable early stopping mechanism over validation dataset. If\n",
            "    validation is not being run, early stopping is disabled.\n",
            "    (default: 'false')\n",
            "  --epochs: how many epochs (complete runs through the train files) to train for\n",
            "    (default: '75')\n",
            "    (an integer)\n",
            "  --epsilon: epsilon parameter of Adam optimizer\n",
            "    (default: '1e-08')\n",
            "    (a number)\n",
            "  --es_epochs: Number of epochs with no improvement after which training will be\n",
            "    stopped. Loss is not stored in the checkpoint so when checkpoint is revived\n",
            "    it starts the loss calculation from start at that point\n",
            "    (default: '25')\n",
            "    (an integer)\n",
            "  --es_min_delta: Minimum change in loss to qualify as an improvement. This\n",
            "    value will also be used in Reduce learning rate on plateau\n",
            "    (default: '0.05')\n",
            "    (a number)\n",
            "  --export_author_id: author of the exported model. GitHub user or organization\n",
            "    name used to uniquely identify the author of this model\n",
            "    (default: 'author')\n",
            "  --export_batch_size: number of elements per batch on the exported graph\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --export_beam_width: default beam width to embed into exported graph\n",
            "    (default: '500')\n",
            "    (an integer)\n",
            "  --export_contact_info: public contact information of the author. Can be an\n",
            "    email address, or a link to a contact form, issue tracker, or discussion\n",
            "    forum. Must provide a way to reach the model authors\n",
            "    (default: '<public contact information of the author. Can be an email\n",
            "    address, or a link to a contact form, issue tracker, or discussion forum.\n",
            "    Must provide a way to reach the model authors>')\n",
            "  --export_description: Freeform description of the model being exported.\n",
            "    Markdown accepted. You can also leave this flag unchanged and edit the\n",
            "    generated .md file directly. Useful things to describe are demographic and\n",
            "    acoustic characteristics of the data used to train the model, any\n",
            "    architectural changes, names of public datasets that were used when\n",
            "    applicable, hyperparameters used for training, evaluation results on\n",
            "    standard benchmark datasets, etc.\n",
            "    (default: '<Freeform description of the model being exported. Markdown\n",
            "    accepted. You can also leave this flag unchanged and edit the generated .md\n",
            "    file directly. Useful things to describe are demographic and acoustic\n",
            "    characteristics of the data used to train the model, any architectural\n",
            "    changes, names of public datasets that were used when applicable,\n",
            "    hyperparameters used for training, evaluation results on standard benchmark\n",
            "    datasets, etc.>')\n",
            "  --export_dir: directory in which exported models are stored - if omitted, the\n",
            "    model won't get exported\n",
            "    (default: '')\n",
            "  --export_file_name: name for the exported model file name\n",
            "    (default: 'output_graph')\n",
            "  --export_language: language the model was trained on - IETF BCP 47 language\n",
            "    tag including at least language, script and region subtags. E.g. \"en-Latn-\n",
            "    UK\" or \"de-Latn-DE\" or \"cmn-Hans-CN\". Include as much info as you can\n",
            "    without loss of precision. For example, if a model is trained on Scottish\n",
            "    English, include the variant subtag: \"en-Latn-GB-Scotland\".\n",
            "    (default: '<language the model was trained on - IETF BCP 47 language tag\n",
            "    including at least language, script and region subtags. E.g. \"en-Latn-UK\" or\n",
            "    \"de-Latn-DE\" or \"cmn-Hans-CN\". Include as much info as you can without loss\n",
            "    of precision. For example, if a model is trained on Scottish English,\n",
            "    include the variant subtag: \"en-Latn-GB-Scotland\".>')\n",
            "  --export_license: SPDX identifier of the license of the exported model. See\n",
            "    https://spdx.org/licenses/. If the license does not have an SPDX identifier,\n",
            "    use the license name.\n",
            "    (default: '<SPDX identifier of the license of the exported model. See\n",
            "    https://spdx.org/licenses/. If the license does not have an SPDX identifier,\n",
            "    use the license name.>')\n",
            "  --export_max_ds_version: maximum DeepSpeech version (inclusive) the exported\n",
            "    model is compatible with\n",
            "    (default: '<maximum DeepSpeech version (inclusive) the exported model is\n",
            "    compatible with>')\n",
            "  --export_min_ds_version: minimum DeepSpeech version (inclusive) the exported\n",
            "    model is compatible with\n",
            "    (default: '<minimum DeepSpeech version (inclusive) the exported model is\n",
            "    compatible with>')\n",
            "  --export_model_name: name of the exported model. Must not contain forward\n",
            "    slashes.\n",
            "    (default: 'model')\n",
            "  --export_model_version: semantic version of the exported model. See\n",
            "    https://semver.org/. This is fully controlled by you as author of the model\n",
            "    and has no required connection with DeepSpeech versions\n",
            "    (default: '0.0.1')\n",
            "  --[no]export_tflite: export a graph ready for TF Lite engine\n",
            "    (default: 'false')\n",
            "  --[no]export_zip: export a TFLite model and package with LM and info.json\n",
            "    (default: 'false')\n",
            "  --feature_cache: cache MFCC features to disk to speed up future training runs\n",
            "    on the same data. This flag specifies the path where cached features\n",
            "    extracted from --train_files will be saved. If empty, or if online\n",
            "    augmentation flags are enabled, caching will be disabled.\n",
            "    (default: '')\n",
            "  --feature_win_len: feature extraction audio window length in milliseconds\n",
            "    (default: '32')\n",
            "    (an integer)\n",
            "  --feature_win_step: feature extraction window step length in milliseconds\n",
            "    (default: '20')\n",
            "    (an integer)\n",
            "  --[no]force_initialize_learning_rate: Force re-initialization of learning rate\n",
            "    which was previously reduced.\n",
            "    (default: 'false')\n",
            "  --inter_op_parallelism_threads: number of inter-op parallelism threads - see\n",
            "    tf.ConfigProto for more details. USE OF THIS FLAG IS UNSUPPORTED\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --intra_op_parallelism_threads: number of intra-op parallelism threads - see\n",
            "    tf.ConfigProto for more details. USE OF THIS FLAG IS UNSUPPORTED\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --learning_rate: learning rate of Adam optimizer\n",
            "    (default: '0.001')\n",
            "    (a number)\n",
            "  --limit_dev: maximum number of elements to use from validation set- 0 means no\n",
            "    limit\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --limit_test: maximum number of elements to use from test set- 0 means no\n",
            "    limit\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --limit_train: maximum number of elements to use from train set - 0 means no\n",
            "    limit\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --lm_alpha: the alpha hyperparameter of the CTC decoder. Language Model\n",
            "    weight.\n",
            "    (default: '0.931289039105002')\n",
            "    (a number)\n",
            "  --lm_alpha_max: the maximum of the alpha hyperparameter of the CTC decoder\n",
            "    explored during hyperparameter optimization. Language Model weight.\n",
            "    (default: '5.0')\n",
            "    (a number)\n",
            "  --lm_beta: the beta hyperparameter of the CTC decoder. Word insertion weight.\n",
            "    (default: '1.1834137581510284')\n",
            "    (a number)\n",
            "  --lm_beta_max: the maximum beta hyperparameter of the CTC decoder explored\n",
            "    during hyperparameter optimization. Word insertion weight.\n",
            "    (default: '5.0')\n",
            "    (a number)\n",
            "  --load_checkpoint_dir: directory in which checkpoints are stored - defaults to\n",
            "    directory \"deepspeech/checkpoints\" within user's data home specified by the\n",
            "    XDG Base Directory Specification\n",
            "    (default: '')\n",
            "  --[no]load_cudnn: Specifying this flag allows one to convert a CuDNN RNN\n",
            "    checkpoint to a checkpoint capable of running on a CPU graph.\n",
            "    (default: 'false')\n",
            "  --load_evaluate: what checkpoint to load for evaluation tasks (test epochs,\n",
            "    model export, single file inference, etc). \"last\" for loading most recent\n",
            "    epoch checkpoint, \"best\" for loading best validation loss checkpoint, \"auto\"\n",
            "    for trying several options.\n",
            "    (default: 'auto')\n",
            "  --load_train: what checkpoint to load before starting the training process.\n",
            "    \"last\" for loading most recent epoch checkpoint, \"best\" for loading best\n",
            "    validation loss checkpoint, \"init\" for initializing a new checkpoint, \"auto\"\n",
            "    for trying several options.\n",
            "    (default: 'auto')\n",
            "  --log_level: log level for console logs - 0: DEBUG, 1: INFO, 2: WARN, 3: ERROR\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --[no]log_placement: whether to log device placement of the operators to the\n",
            "    console\n",
            "    (default: 'false')\n",
            "  --max_to_keep: number of checkpoint files to keep - default value is 5\n",
            "    (default: '5')\n",
            "    (an integer)\n",
            "  --n_hidden: layer width to use when initialising layers\n",
            "    (default: '2048')\n",
            "    (an integer)\n",
            "  --n_steps: how many timesteps to process at once by the export graph, higher\n",
            "    values mean more latency\n",
            "    (default: '16')\n",
            "    (an integer)\n",
            "  --n_trials: the number of trials to run during hyperparameter optimization.\n",
            "    (default: '2400')\n",
            "    (an integer)\n",
            "  --one_shot_infer: one-shot inference mode: specify a wav file and the script\n",
            "    will load the checkpoint and perform inference on it.\n",
            "    (default: '')\n",
            "  --plateau_epochs: Number of epochs to consider for RLROP. Has to be smaller\n",
            "    than es_epochs from early stopping\n",
            "    (default: '10')\n",
            "    (an integer)\n",
            "  --plateau_reduction: Multiplicative factor to apply to the current learning\n",
            "    rate if a plateau has occurred.\n",
            "    (default: '0.1')\n",
            "    (a number)\n",
            "  --random_seed: default random seed that is used to initialize variables\n",
            "    (default: '4568')\n",
            "    (an integer)\n",
            "  --read_buffer: buffer-size for reading samples from datasets (supports file-\n",
            "    size suffixes KB, MB, GB, TB)\n",
            "    (default: '1MB')\n",
            "  --[no]reduce_lr_on_plateau: Enable reducing the learning rate if a plateau is\n",
            "    reached. This is the case if the validation loss did not improve for some\n",
            "    epochs.\n",
            "    (default: 'false')\n",
            "  --relu_clip: ReLU clipping value for non-recurrent layers\n",
            "    (default: '20.0')\n",
            "    (a number)\n",
            "  --[no]remove_export: whether to remove old exported models\n",
            "    (default: 'false')\n",
            "  --report_count: number of phrases for each of best WER, median WER and worst\n",
            "    WER to print out during a WER report\n",
            "    (default: '5')\n",
            "    (an integer)\n",
            "  --save_checkpoint_dir: directory to which checkpoints are saved - defaults to\n",
            "    directory \"deepspeech/checkpoints\" within user's data home specified by the\n",
            "    XDG Base Directory Specification\n",
            "    (default: '')\n",
            "  --scorer: Alias for --scorer_path.\n",
            "    (default: 'data/lm/kenlm.scorer')\n",
            "  --scorer_path: path to the external scorer file created with\n",
            "    data/lm/generate_package.py\n",
            "    (default: 'data/lm/kenlm.scorer')\n",
            "  --[no]show_progressbar: Show progress for training, validation and testing\n",
            "    processes. Log level should be > 0.\n",
            "    (default: 'true')\n",
            "  --summary_dir: target directory for TensorBoard summaries - defaults to\n",
            "    directory \"deepspeech/summaries\" within user's data home specified by the\n",
            "    XDG Base Directory Specification\n",
            "    (default: '')\n",
            "  --test_batch_size: number of elements in a test batch\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --test_files: comma separated list of files specifying the dataset used for\n",
            "    testing. Multiple files will get merged. If empty, the model will not be\n",
            "    tested.\n",
            "    (default: '')\n",
            "  --test_output_file: path to a file to save all src/decoded/distance/loss\n",
            "    tuples generated during a test epoch\n",
            "    (default: '')\n",
            "  --train_batch_size: number of elements in a training batch\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --[no]train_cudnn: use CuDNN RNN backend for training on GPU. Note that\n",
            "    checkpoints created with this flag can only be used with CuDNN RNN, i.e.\n",
            "    fine tuning on a CPU device will not work\n",
            "    (default: 'false')\n",
            "  --train_files: comma separated list of files specifying the dataset used for\n",
            "    training. Multiple files will get merged. If empty, training will not be\n",
            "    run.\n",
            "    (default: '')\n",
            "  --[no]use_allow_growth: use Allow Growth flag which will allocate only\n",
            "    required amount of GPU memory and prevent full allocation of available GPU\n",
            "    memory\n",
            "    (default: 'false')\n",
            "  --[no]utf8: enable UTF-8 mode. When this is used the model outputs UTF-8\n",
            "    sequences directly rather than using an alphabet mapping.\n",
            "    (default: 'false')\n",
            "\n",
            "tensorflow.python.ops.parallel_for.pfor:\n",
            "  --[no]op_conversion_fallback_to_while_loop: If true, falls back to using a\n",
            "    while loop for ops for which a converter is not defined.\n",
            "    (default: 'false')\n",
            "\n",
            "absl.flags:\n",
            "  --flagfile: Insert flag definitions from the given file into the command line.\n",
            "    (default: '')\n",
            "  --undefok: comma-separated list of flag names that it is okay to specify on\n",
            "    the command line even if the program does not define a flag with that name.\n",
            "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\n",
            "    format.\n",
            "    (default: '')\n",
            "+ [ ! -f DeepSpeech.py ]\n",
            "+ [ ! -f data/ldc93s1/ldc93s1.csv ]\n",
            "+ echo Downloading and preprocessing LDC93S1 example data, saving in ./data/ldc93s1.\n",
            "Downloading and preprocessing LDC93S1 example data, saving in ./data/ldc93s1.\n",
            "+ python -u bin/import_ldc93s1.py ./data/ldc93s1\n",
            "No path \"./data/ldc93s1\" - creating ...\n",
            "No archive \"./data/ldc93s1/LDC93S1.wav\" - downloading...\n",
            "Progress |                                                      | N/A% completedNo archive \"./data/ldc93s1/LDC93S1.txt\" - downloading...\n",
            "Progress |######################################################| 100% completed\n",
            "Progress |######################################################| 100% completed\n",
            "+ [ -d  ]\n",
            "+ python -c from xdg import BaseDirectory as xdg; print(xdg.save_data_path(\"deepspeech/ldc93s1\"))\n",
            "+ checkpoint_dir=/root/.local/share/deepspeech/ldc93s1\n",
            "+ export CUDA_VISIBLE_DEVICES=0\n",
            "+ python -u DeepSpeech.py --noshow_progressbar --train_files data/ldc93s1/ldc93s1.csv --test_files data/ldc93s1/ldc93s1.csv --train_batch_size 1 --test_batch_size 1 --n_hidden 100 --epochs 200 --checkpoint_dir /root/.local/share/deepspeech/ldc93s1\n",
            "I Could not find best validating checkpoint.\n",
            "I Could not find most recent checkpoint.\n",
            "I Initializing all variables.\n",
            "I STARTING Optimization\n",
            "I Training epoch 0...\n",
            "I Finished training epoch 0 - loss: 367.066650\n",
            "I Training epoch 1...\n",
            "I Finished training epoch 1 - loss: 335.958954\n",
            "I Training epoch 2...\n",
            "I Finished training epoch 2 - loss: 305.713654\n",
            "I Training epoch 3...\n",
            "I Finished training epoch 3 - loss: 276.875458\n",
            "I Training epoch 4...\n",
            "I Finished training epoch 4 - loss: 246.741196\n",
            "I Training epoch 5...\n",
            "I Finished training epoch 5 - loss: 223.746674\n",
            "I Training epoch 6...\n",
            "I Finished training epoch 6 - loss: 201.275513\n",
            "I Training epoch 7...\n",
            "I Finished training epoch 7 - loss: 183.947662\n",
            "I Training epoch 8...\n",
            "I Finished training epoch 8 - loss: 173.568863\n",
            "I Training epoch 9...\n",
            "I Finished training epoch 9 - loss: 170.314667\n",
            "I Training epoch 10...\n",
            "I Finished training epoch 10 - loss: 171.134796\n",
            "I Training epoch 11...\n",
            "I Finished training epoch 11 - loss: 172.941055\n",
            "I Training epoch 12...\n",
            "I Finished training epoch 12 - loss: 172.322754\n",
            "I Training epoch 13...\n",
            "I Finished training epoch 13 - loss: 167.915695\n",
            "I Training epoch 14...\n",
            "I Finished training epoch 14 - loss: 163.602493\n",
            "I Training epoch 15...\n",
            "I Finished training epoch 15 - loss: 158.262619\n",
            "I Training epoch 16...\n",
            "I Finished training epoch 16 - loss: 153.835922\n",
            "I Training epoch 17...\n",
            "I Finished training epoch 17 - loss: 151.067108\n",
            "I Training epoch 18...\n",
            "I Finished training epoch 18 - loss: 149.761795\n",
            "I Training epoch 19...\n",
            "I Finished training epoch 19 - loss: 148.875000\n",
            "I Training epoch 20...\n",
            "I Finished training epoch 20 - loss: 148.358414\n",
            "I Training epoch 21...\n",
            "I Finished training epoch 21 - loss: 146.951019\n",
            "I Training epoch 22...\n",
            "I Finished training epoch 22 - loss: 145.639877\n",
            "I Training epoch 23...\n",
            "I Finished training epoch 23 - loss: 144.072495\n",
            "I Training epoch 24...\n",
            "I Finished training epoch 24 - loss: 143.343948\n",
            "I Training epoch 25...\n",
            "I Finished training epoch 25 - loss: 141.214462\n",
            "I Training epoch 26...\n",
            "I Finished training epoch 26 - loss: 140.702576\n",
            "I Training epoch 27...\n",
            "I Finished training epoch 27 - loss: 140.455612\n",
            "I Training epoch 28...\n",
            "I Finished training epoch 28 - loss: 140.454453\n",
            "I Training epoch 29...\n",
            "I Finished training epoch 29 - loss: 140.500259\n",
            "I Training epoch 30...\n",
            "I Finished training epoch 30 - loss: 139.666534\n",
            "I Training epoch 31...\n",
            "I Finished training epoch 31 - loss: 139.356476\n",
            "I Training epoch 32...\n",
            "I Finished training epoch 32 - loss: 138.982162\n",
            "I Training epoch 33...\n",
            "I Finished training epoch 33 - loss: 139.124313\n",
            "I Training epoch 34...\n",
            "I Finished training epoch 34 - loss: 137.839233\n",
            "I Training epoch 35...\n",
            "I Finished training epoch 35 - loss: 138.409424\n",
            "I Training epoch 36...\n",
            "I Finished training epoch 36 - loss: 137.484589\n",
            "I Training epoch 37...\n",
            "I Finished training epoch 37 - loss: 137.517639\n",
            "I Training epoch 38...\n",
            "I Finished training epoch 38 - loss: 137.731949\n",
            "I Training epoch 39...\n",
            "I Finished training epoch 39 - loss: 136.841354\n",
            "I Training epoch 40...\n",
            "I Finished training epoch 40 - loss: 137.879440\n",
            "I Training epoch 41...\n",
            "I Finished training epoch 41 - loss: 137.064041\n",
            "I Training epoch 42...\n",
            "I Finished training epoch 42 - loss: 136.704926\n",
            "I Training epoch 43...\n",
            "I Finished training epoch 43 - loss: 135.753464\n",
            "I Training epoch 44...\n",
            "I Finished training epoch 44 - loss: 136.036591\n",
            "I Training epoch 45...\n",
            "I Finished training epoch 45 - loss: 135.115097\n",
            "I Training epoch 46...\n",
            "I Finished training epoch 46 - loss: 135.154922\n",
            "I Training epoch 47...\n",
            "I Finished training epoch 47 - loss: 135.530579\n",
            "I Training epoch 48...\n",
            "I Finished training epoch 48 - loss: 134.463150\n",
            "I Training epoch 49...\n",
            "I Finished training epoch 49 - loss: 134.542206\n",
            "I Training epoch 50...\n",
            "I Finished training epoch 50 - loss: 134.224503\n",
            "I Training epoch 51...\n",
            "I Finished training epoch 51 - loss: 134.035141\n",
            "I Training epoch 52...\n",
            "I Finished training epoch 52 - loss: 133.070786\n",
            "I Training epoch 53...\n",
            "I Finished training epoch 53 - loss: 132.992844\n",
            "I Training epoch 54...\n",
            "I Finished training epoch 54 - loss: 132.539246\n",
            "I Training epoch 55...\n",
            "I Finished training epoch 55 - loss: 131.742630\n",
            "I Training epoch 56...\n",
            "I Finished training epoch 56 - loss: 132.095596\n",
            "I Training epoch 57...\n",
            "I Finished training epoch 57 - loss: 131.134155\n",
            "I Training epoch 58...\n",
            "I Finished training epoch 58 - loss: 130.947495\n",
            "I Training epoch 59...\n",
            "I Finished training epoch 59 - loss: 129.932083\n",
            "I Training epoch 60...\n",
            "I Finished training epoch 60 - loss: 130.284637\n",
            "I Training epoch 61...\n",
            "I Finished training epoch 61 - loss: 128.421539\n",
            "I Training epoch 62...\n",
            "I Finished training epoch 62 - loss: 128.892166\n",
            "I Training epoch 63...\n",
            "I Finished training epoch 63 - loss: 128.911240\n",
            "I Training epoch 64...\n",
            "I Finished training epoch 64 - loss: 127.905983\n",
            "I Training epoch 65...\n",
            "I Finished training epoch 65 - loss: 127.834442\n",
            "I Training epoch 66...\n",
            "I Finished training epoch 66 - loss: 126.732635\n",
            "I Training epoch 67...\n",
            "I Finished training epoch 67 - loss: 125.436935\n",
            "I Training epoch 68...\n",
            "I Finished training epoch 68 - loss: 125.626709\n",
            "I Training epoch 69...\n",
            "I Finished training epoch 69 - loss: 124.485619\n",
            "I Training epoch 70...\n",
            "I Finished training epoch 70 - loss: 124.664413\n",
            "I Training epoch 71...\n",
            "I Finished training epoch 71 - loss: 124.019722\n",
            "I Training epoch 72...\n",
            "I Finished training epoch 72 - loss: 123.248634\n",
            "I Training epoch 73...\n",
            "I Finished training epoch 73 - loss: 121.275337\n",
            "I Training epoch 74...\n",
            "I Finished training epoch 74 - loss: 119.450951\n",
            "I Training epoch 75...\n",
            "I Finished training epoch 75 - loss: 119.557671\n",
            "I Training epoch 76...\n",
            "I Finished training epoch 76 - loss: 118.746552\n",
            "I Training epoch 77...\n",
            "I Finished training epoch 77 - loss: 117.597595\n",
            "I Training epoch 78...\n",
            "I Finished training epoch 78 - loss: 117.123535\n",
            "I Training epoch 79...\n",
            "I Finished training epoch 79 - loss: 116.165962\n",
            "I Training epoch 80...\n",
            "I Finished training epoch 80 - loss: 115.393738\n",
            "I Training epoch 81...\n",
            "I Finished training epoch 81 - loss: 114.566872\n",
            "I Training epoch 82...\n",
            "I Finished training epoch 82 - loss: 113.387192\n",
            "I Training epoch 83...\n",
            "I Finished training epoch 83 - loss: 112.268860\n",
            "I Training epoch 84...\n",
            "I Finished training epoch 84 - loss: 111.142494\n",
            "I Training epoch 85...\n",
            "I Finished training epoch 85 - loss: 110.834991\n",
            "I Training epoch 86...\n",
            "I Finished training epoch 86 - loss: 109.041130\n",
            "I Training epoch 87...\n",
            "I Finished training epoch 87 - loss: 108.808762\n",
            "I Training epoch 88...\n",
            "I Finished training epoch 88 - loss: 108.246193\n",
            "I Training epoch 89...\n",
            "I Finished training epoch 89 - loss: 106.411720\n",
            "I Training epoch 90...\n",
            "I Finished training epoch 90 - loss: 105.670853\n",
            "I Training epoch 91...\n",
            "I Finished training epoch 91 - loss: 105.178749\n",
            "I Training epoch 92...\n",
            "I Finished training epoch 92 - loss: 104.091278\n",
            "I Training epoch 93...\n",
            "I Finished training epoch 93 - loss: 103.162781\n",
            "I Training epoch 94...\n",
            "I Finished training epoch 94 - loss: 100.453934\n",
            "I Training epoch 95...\n",
            "I Finished training epoch 95 - loss: 99.920265\n",
            "I Training epoch 96...\n",
            "I Finished training epoch 96 - loss: 99.940849\n",
            "I Training epoch 97...\n",
            "I Finished training epoch 97 - loss: 99.675156\n",
            "I Training epoch 98...\n",
            "I Finished training epoch 98 - loss: 97.270515\n",
            "I Training epoch 99...\n",
            "I Finished training epoch 99 - loss: 96.605690\n",
            "I Training epoch 100...\n",
            "I Finished training epoch 100 - loss: 95.377518\n",
            "I Training epoch 101...\n",
            "I Finished training epoch 101 - loss: 96.649467\n",
            "I Training epoch 102...\n",
            "I Finished training epoch 102 - loss: 94.238235\n",
            "I Training epoch 103...\n",
            "I Finished training epoch 103 - loss: 92.571045\n",
            "I Training epoch 104...\n",
            "I Finished training epoch 104 - loss: 92.716522\n",
            "I Training epoch 105...\n",
            "I Finished training epoch 105 - loss: 93.602814\n",
            "I Training epoch 106...\n",
            "I Finished training epoch 106 - loss: 91.326096\n",
            "I Training epoch 107...\n",
            "I Finished training epoch 107 - loss: 89.614944\n",
            "I Training epoch 108...\n",
            "I Finished training epoch 108 - loss: 89.083282\n",
            "I Training epoch 109...\n",
            "I Finished training epoch 109 - loss: 88.684372\n",
            "I Training epoch 110...\n",
            "I Finished training epoch 110 - loss: 87.320976\n",
            "I Training epoch 111...\n",
            "I Finished training epoch 111 - loss: 84.628654\n",
            "I Training epoch 112...\n",
            "I Finished training epoch 112 - loss: 85.598076\n",
            "I Training epoch 113...\n",
            "I Finished training epoch 113 - loss: 84.124870\n",
            "I Training epoch 114...\n",
            "I Finished training epoch 114 - loss: 82.693710\n",
            "I Training epoch 115...\n",
            "I Finished training epoch 115 - loss: 80.154167\n",
            "I Training epoch 116...\n",
            "I Finished training epoch 116 - loss: 80.540680\n",
            "I Training epoch 117...\n",
            "I Finished training epoch 117 - loss: 80.287155\n",
            "I Training epoch 118...\n",
            "I Finished training epoch 118 - loss: 79.430565\n",
            "I Training epoch 119...\n",
            "I Finished training epoch 119 - loss: 79.223228\n",
            "I Training epoch 120...\n",
            "I Finished training epoch 120 - loss: 80.276245\n",
            "I Training epoch 121...\n",
            "I Finished training epoch 121 - loss: 77.387543\n",
            "I Training epoch 122...\n",
            "I Finished training epoch 122 - loss: 76.664024\n",
            "I Training epoch 123...\n",
            "I Finished training epoch 123 - loss: 73.474052\n",
            "I Training epoch 124...\n",
            "I Finished training epoch 124 - loss: 71.370598\n",
            "I Training epoch 125...\n",
            "I Finished training epoch 125 - loss: 74.046951\n",
            "I Training epoch 126...\n",
            "I Finished training epoch 126 - loss: 72.806770\n",
            "I Training epoch 127...\n",
            "I Finished training epoch 127 - loss: 69.949966\n",
            "I Training epoch 128...\n",
            "I Finished training epoch 128 - loss: 71.310066\n",
            "I Training epoch 129...\n",
            "I Finished training epoch 129 - loss: 67.839203\n",
            "I Training epoch 130...\n",
            "I Finished training epoch 130 - loss: 66.509369\n",
            "I Training epoch 131...\n",
            "I Finished training epoch 131 - loss: 65.894951\n",
            "I Training epoch 132...\n",
            "I Finished training epoch 132 - loss: 66.249466\n",
            "I Training epoch 133...\n",
            "I Finished training epoch 133 - loss: 63.494129\n",
            "I Training epoch 134...\n",
            "I Finished training epoch 134 - loss: 63.342133\n",
            "I Training epoch 135...\n",
            "I Finished training epoch 135 - loss: 61.325085\n",
            "I Training epoch 136...\n",
            "I Finished training epoch 136 - loss: 62.618038\n",
            "I Training epoch 137...\n",
            "I Finished training epoch 137 - loss: 61.237679\n",
            "I Training epoch 138...\n",
            "I Finished training epoch 138 - loss: 61.133820\n",
            "I Training epoch 139...\n",
            "I Finished training epoch 139 - loss: 58.447010\n",
            "I Training epoch 140...\n",
            "I Finished training epoch 140 - loss: 58.327229\n",
            "I Training epoch 141...\n",
            "I Finished training epoch 141 - loss: 56.136223\n",
            "I Training epoch 142...\n",
            "I Finished training epoch 142 - loss: 55.970070\n",
            "I Training epoch 143...\n",
            "I Finished training epoch 143 - loss: 53.485256\n",
            "I Training epoch 144...\n",
            "I Finished training epoch 144 - loss: 54.772285\n",
            "I Training epoch 145...\n",
            "I Finished training epoch 145 - loss: 52.938396\n",
            "I Training epoch 146...\n",
            "I Finished training epoch 146 - loss: 50.573238\n",
            "I Training epoch 147...\n",
            "I Finished training epoch 147 - loss: 49.607735\n",
            "I Training epoch 148...\n",
            "I Finished training epoch 148 - loss: 50.267349\n",
            "I Training epoch 149...\n",
            "I Finished training epoch 149 - loss: 48.036034\n",
            "I Training epoch 150...\n",
            "I Finished training epoch 150 - loss: 48.951286\n",
            "I Training epoch 151...\n",
            "I Finished training epoch 151 - loss: 44.666679\n",
            "I Training epoch 152...\n",
            "I Finished training epoch 152 - loss: 43.889297\n",
            "I Training epoch 153...\n",
            "I Finished training epoch 153 - loss: 45.051468\n",
            "I Training epoch 154...\n",
            "I Finished training epoch 154 - loss: 42.938663\n",
            "I Training epoch 155...\n",
            "I Finished training epoch 155 - loss: 40.590313\n",
            "I Training epoch 156...\n",
            "I Finished training epoch 156 - loss: 42.401993\n",
            "I Training epoch 157...\n",
            "I Finished training epoch 157 - loss: 42.439877\n",
            "I Training epoch 158...\n",
            "I Finished training epoch 158 - loss: 43.351261\n",
            "I Training epoch 159...\n",
            "I Finished training epoch 159 - loss: 37.512413\n",
            "I Training epoch 160...\n",
            "I Finished training epoch 160 - loss: 36.059143\n",
            "I Training epoch 161...\n",
            "I Finished training epoch 161 - loss: 37.463734\n",
            "I Training epoch 162...\n",
            "I Finished training epoch 162 - loss: 35.153233\n",
            "I Training epoch 163...\n",
            "I Finished training epoch 163 - loss: 35.160751\n",
            "I Training epoch 164...\n",
            "I Finished training epoch 164 - loss: 35.843098\n",
            "I Training epoch 165...\n",
            "I Finished training epoch 165 - loss: 34.098213\n",
            "I Training epoch 166...\n",
            "I Finished training epoch 166 - loss: 32.982941\n",
            "I Training epoch 167...\n",
            "I Finished training epoch 167 - loss: 30.029423\n",
            "I Training epoch 168...\n",
            "I Finished training epoch 168 - loss: 32.286015\n",
            "I Training epoch 169...\n",
            "I Finished training epoch 169 - loss: 29.335468\n",
            "I Training epoch 170...\n",
            "I Finished training epoch 170 - loss: 32.769012\n",
            "I Training epoch 171...\n",
            "I Finished training epoch 171 - loss: 26.960293\n",
            "I Training epoch 172...\n",
            "I Finished training epoch 172 - loss: 27.153694\n",
            "I Training epoch 173...\n",
            "I Finished training epoch 173 - loss: 29.760881\n",
            "I Training epoch 174...\n",
            "I Finished training epoch 174 - loss: 24.882317\n",
            "I Training epoch 175...\n",
            "I Finished training epoch 175 - loss: 23.415195\n",
            "I Training epoch 176...\n",
            "I Finished training epoch 176 - loss: 23.808023\n",
            "I Training epoch 177...\n",
            "I Finished training epoch 177 - loss: 23.988008\n",
            "I Training epoch 178...\n",
            "I Finished training epoch 178 - loss: 22.071175\n",
            "I Training epoch 179...\n",
            "I Finished training epoch 179 - loss: 23.183331\n",
            "I Training epoch 180...\n",
            "I Finished training epoch 180 - loss: 22.368845\n",
            "I Training epoch 181...\n",
            "I Finished training epoch 181 - loss: 23.905350\n",
            "I Training epoch 182...\n",
            "I Finished training epoch 182 - loss: 22.529751\n",
            "I Training epoch 183...\n",
            "I Finished training epoch 183 - loss: 19.598160\n",
            "I Training epoch 184...\n",
            "I Finished training epoch 184 - loss: 18.915634\n",
            "I Training epoch 185...\n",
            "I Finished training epoch 185 - loss: 19.095592\n",
            "I Training epoch 186...\n",
            "I Finished training epoch 186 - loss: 17.935793\n",
            "I Training epoch 187...\n",
            "I Finished training epoch 187 - loss: 18.719574\n",
            "I Training epoch 188...\n",
            "I Finished training epoch 188 - loss: 18.184998\n",
            "I Training epoch 189...\n",
            "I Finished training epoch 189 - loss: 19.101841\n",
            "I Training epoch 190...\n",
            "I Finished training epoch 190 - loss: 17.448032\n",
            "I Training epoch 191...\n",
            "I Finished training epoch 191 - loss: 16.626257\n",
            "I Training epoch 192...\n",
            "I Finished training epoch 192 - loss: 14.765353\n",
            "I Training epoch 193...\n",
            "I Finished training epoch 193 - loss: 16.804764\n",
            "I Training epoch 194...\n",
            "I Finished training epoch 194 - loss: 12.736980\n",
            "I Training epoch 195...\n",
            "I Finished training epoch 195 - loss: 13.169114\n",
            "I Training epoch 196...\n",
            "I Finished training epoch 196 - loss: 11.635065\n",
            "I Training epoch 197...\n",
            "I Finished training epoch 197 - loss: 12.475252\n",
            "I Training epoch 198...\n",
            "I Finished training epoch 198 - loss: 12.402353\n",
            "I Training epoch 199...\n",
            "I Finished training epoch 199 - loss: 13.545377\n",
            "I FINISHED optimization in 0:01:10.716976\n",
            "I Could not find best validating checkpoint.\n",
            "I Loading most recent checkpoint from /root/.local/share/deepspeech/ldc93s1/train-200\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on data/ldc93s1/ldc93s1.csv\n",
            "I Test epoch...\n",
            "Test on data/ldc93s1/ldc93s1.csv - WER: 0.000000, CER: 0.000000, loss: 7.990866\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 7.990866\n",
            " - wav: file:///content/DeepSpeech/data/ldc93s1/LDC93S1.wav\n",
            " - src: \"she had your dark suit in greasy wash water all year\"\n",
            " - res: \"she had your dark suit in greasy wash water all year\"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 7.990866\n",
            " - wav: file:///content/DeepSpeech/data/ldc93s1/LDC93S1.wav\n",
            " - src: \"she had your dark suit in greasy wash water all year\"\n",
            " - res: \"she had your dark suit in greasy wash water all year\"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 7.990866\n",
            " - wav: file:///content/DeepSpeech/data/ldc93s1/LDC93S1.wav\n",
            " - src: \"she had your dark suit in greasy wash water all year\"\n",
            " - res: \"she had your dark suit in greasy wash water all year\"\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhFyawDY4qw8",
        "colab_type": "code",
        "outputId": "b7651a91-fc95-4b42-9250-0b1ff088dc95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%cd '/content/DeepSpeech/'\n",
        "!python3 '/content/DeepSpeech/DeepSpeech.py' \\\n",
        "    --n_hidden 2048 \\\n",
        "    --drop_source_layers 1 \\\n",
        "    --epochs 3 \\\n",
        "    --checkpoint_dir '/content/drive/My Drive/transfer_learning/deepspeech-0.7.0-checkpoint/' \\\n",
        "    --alphabet_config_path '/content/drive/My Drive/transfer_learning/alphabet_training.txt'\\\n",
        "    --train_files   '/content/drive/My Drive/ytd_project/testing/excel files/tessstttt.csv' \\\n",
        "    --dev_files   '/content/drive/My Drive/ytd_project/testing/excel files/dev2.csv' \\\n",
        "    --test_files  '/content/drive/My Drive/ytd_project/testing/excel files/test2.csv' \\\n",
        "    --summary_dir '/content/drive/My Drive/transfer_learning/summary_dir' \\\n",
        "    --load_cudnn \\\n",
        "\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/DeepSpeech/'\n",
            "/content\n",
            "python3: can't open file '/content/DeepSpeech/DeepSpeech.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8bQL9_jeOK0",
        "colab_type": "code",
        "outputId": "8b9ba5b4-14a5-4404-85b2-790055d89793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd '/content/DeepSpeech/'\n",
        "!python3 '/content/DeepSpeech/DeepSpeech.py' \\\n",
        "    --n_hidden 2048 \\\n",
        "    --checkpoint_dir '/content/drive/My Drive/transfer_learning/deepspeech-0.7.0-checkpoint/' \\\n",
        "    --epochs 1 \\\n",
        "    --alphabet_config_path '/content/drive/My Drive/transfer_learning/alphabet_training.txt'\\\n",
        "    --train_files   '/content/drive/My Drive/ytd_project/testing/excel files/train4.csv' \\\n",
        "    --dev_files   '/content/drive/My Drive/ytd_project/testing/excel files/dev2.csv' \\\n",
        "    --test_files  '/content/drive/My Drive/ytd_project/testing/excel files/test2.csv' \\\n",
        "    --export_dir '/content/drive/My Drive/transfer_learning/exported_model' \\\n",
        "    --load_cudnn \\"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DeepSpeech\n",
            "I Loading best validating checkpoint from /content/drive/My Drive/transfer_learning/deepspeech-0.7.0-checkpoint/best_dev-747750\n",
            "I Loading variable from checkpoint: beta1_power\n",
            "I Loading variable from checkpoint: beta2_power\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/bias/Adam\n",
            "I Loading variable from checkpoint: layer_6/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "I Loading variable from checkpoint: layer_6/weights/Adam\n",
            "I Loading variable from checkpoint: layer_6/weights/Adam_1\n",
            "I Loading variable from checkpoint: learning_rate\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:00 | Steps: 0 | Loss: 0.000000        WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc0.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_0_0_1.1bankingZ08Xnc0.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_10_0_1.1bankingZ08Xnc0.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc1.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:02 | Steps: 1 | Loss: 172.666931      WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_0_0_1.1bankingZ08Xnc1.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:02 | Steps: 2 | Loss: 172.652039      WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_10_0_1.1bankingZ08Xnc1.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:02 | Steps: 3 | Loss: 172.629649      WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc2.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:03 | Steps: 4 | Loss: 187.516273      WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_0_0_1.1bankingZ08Xnc2.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:03 | Steps: 5 | Loss: 196.142349      WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_10_0_1.1bankingZ08Xnc2.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:03 | Steps: 6 | Loss: 201.972361      WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc3.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:03 | Steps: 7 | Loss: 218.195330      WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_0_0_1.1bankingZ08Xnc3.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:03 | Steps: 8 | Loss: 230.085346      WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_10_0_1.1bankingZ08Xnc3.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:04 | Steps: 9 | Loss: 239.228811      WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc4.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:04 | Steps: 10 | Loss: 236.804559     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_0_0_1.1bankingZ08Xnc4.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:04 | Steps: 11 | Loss: 234.666204     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_10_0_1.1bankingZ08Xnc4.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:04 | Steps: 12 | Loss: 232.814531     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc5.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:05 | Steps: 13 | Loss: 232.898669     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_0_0_1.1bankingZ08Xnc5.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:05 | Steps: 14 | Loss: 232.911534     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_10_0_1.1bankingZ08Xnc5.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:05 | Steps: 15 | Loss: 232.877708     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc6.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:05 | Steps: 16 | Loss: 234.019820     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_0_0_1.1bankingZ08Xnc6.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:06 | Steps: 17 | Loss: 235.027175     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_10_0_1.1bankingZ08Xnc6.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:06 | Steps: 18 | Loss: 235.913499     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc8.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:06 | Steps: 19 | Loss: 232.811007     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_0_0_1.1bankingZ08Xnc8.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:06 | Steps: 20 | Loss: 230.027869     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_10_0_1.1bankingZ08Xnc8.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:07 | Steps: 21 | Loss: 227.406884     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc9.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:07 | Steps: 22 | Loss: 223.071436     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_0_0_1.1bankingZ08Xnc9.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:07 | Steps: 23 | Loss: 219.043509     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_10_0_1.1bankingZ08Xnc9.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:07 | Steps: 24 | Loss: 215.288684     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc10.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:08 | Steps: 25 | Loss: 217.600882     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_0_0_1.1bankingZ08Xnc10.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:08 | Steps: 26 | Loss: 219.732489     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_10_0_1.1bankingZ08Xnc10.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:08 | Steps: 27 | Loss: 221.736434     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc11.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:08 | Steps: 28 | Loss: 222.361864     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_0_0_1.1bankingZ08Xnc11.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:09 | Steps: 29 | Loss: 223.171648     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_10_0_1.1bankingZ08Xnc11.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:09 | Steps: 30 | Loss: 223.893745     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc12.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:09 | Steps: 31 | Loss: 225.151004     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_0_0_1.1bankingZ08Xnc12.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:09 | Steps: 32 | Loss: 226.268170     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_10_0_1.1bankingZ08Xnc12.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:09 | Steps: 33 | Loss: 227.387330     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc13.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:10 | Steps: 34 | Loss: 230.156914     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_0_0_1.1bankingZ08Xnc13.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:10 | Steps: 35 | Loss: 232.649537     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_10_0_1.1bankingZ08Xnc13.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:10 | Steps: 36 | Loss: 234.923628     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc14.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:10 | Steps: 37 | Loss: 235.205874     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_0_0_1.1bankingZ08Xnc14.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:11 | Steps: 38 | Loss: 235.472328     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_10_0_1.1bankingZ08Xnc14.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:11 | Steps: 39 | Loss: 235.668320     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc15.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:11 | Steps: 40 | Loss: 235.796741     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_0_0_1.1bankingZ08Xnc15.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:11 | Steps: 41 | Loss: 235.893846     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_10_0_1.1bankingZ08Xnc15.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:12 | Steps: 42 | Loss: 235.989476     WARNING: sample rate of sample \"/content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingZ08Xnc16.wav\" ( 8000 ) does not match FLAGS.audio_sample_rate. This can lead to incorrect results.\n",
            "Process ForkPoolWorker-2:\n",
            "Process ForkPoolWorker-1:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "I FINISHED optimization in 0:00:12.658855\n",
            "I Loading best validating checkpoint from /content/drive/My Drive/transfer_learning/deepspeech-0.7.0-checkpoint/best_dev-747750\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on /content/drive/My Drive/ytd_project/testing/excel files/test2.csv\n",
            "Test epoch | Steps: 13 | Elapsed Time: 0:00:04                                  \n",
            "Test on /content/drive/My Drive/ytd_project/testing/excel files/test2.csv - WER: 1.000000, CER: 0.935551, loss: 132.235123\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.918919, loss: 135.194504\n",
            " - wav: file:///content/drive/My Drive/ytd_project/testing/transformed files/_-5_-10_0.95bankingLui4Bc1.wav\n",
            " - src: \"hello and welcome back to how to bind\"\n",
            " - res: \"one\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.918919, loss: 135.194504\n",
            " - wav: file:///content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_0.95bankingLui4Bc1.wav\n",
            " - src: \"hello and welcome back to how to bind\"\n",
            " - res: \"one\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.918919, loss: 135.194504\n",
            " - wav: file:///content/drive/My Drive/ytd_project/testing/transformed files/_-5_5_0.95bankingLui4Bc1.wav\n",
            " - src: \"hello and welcome back to how to bind\"\n",
            " - res: \"one\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.918919, loss: 135.194504\n",
            " - wav: file:///content/drive/My Drive/ytd_project/testing/transformed files/_0_-10_0.95bankingLui4Bc1.wav\n",
            " - src: \"hello and welcome back to how to bind\"\n",
            " - res: \"one\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.918919, loss: 135.194504\n",
            " - wav: file:///content/drive/My Drive/ytd_project/testing/transformed files/_0_0_0.95bankingLui4Bc1.wav\n",
            " - src: \"hello and welcome back to how to bind\"\n",
            " - res: \"one\"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.918919, loss: 135.194504\n",
            " - wav: file:///content/drive/My Drive/ytd_project/testing/transformed files/_0_0_0.95bankingLui4Bc1.wav\n",
            " - src: \"hello and welcome back to how to bind\"\n",
            " - res: \"one\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.918919, loss: 132.582611\n",
            " - wav: file:///content/drive/My Drive/ytd_project/testing/transformed files/_-5_-10_0bankingLui4Bc1.wav\n",
            " - src: \"hello and welcome back to how to bind\"\n",
            " - res: \"one\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.918919, loss: 132.582611\n",
            " - wav: file:///content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_0bankingLui4Bc1.wav\n",
            " - src: \"hello and welcome back to how to bind\"\n",
            " - res: \"one\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.918919, loss: 132.582611\n",
            " - wav: file:///content/drive/My Drive/ytd_project/testing/transformed files/_-5_5_0bankingLui4Bc1.wav\n",
            " - src: \"hello and welcome back to how to bind\"\n",
            " - res: \"one\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.918919, loss: 132.582611\n",
            " - wav: file:///content/drive/My Drive/ytd_project/testing/transformed files/_0_-10_0bankingLui4Bc1.wav\n",
            " - src: \"hello and welcome back to how to bind\"\n",
            " - res: \"one\"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.918919, loss: 132.582611\n",
            " - wav: file:///content/drive/My Drive/ytd_project/testing/transformed files/_0_-10_0bankingLui4Bc1.wav\n",
            " - src: \"hello and welcome back to how to bind\"\n",
            " - res: \"one\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.972973, loss: 128.188446\n",
            " - wav: file:///content/drive/My Drive/ytd_project/testing/transformed files/_-5_0_1.1bankingLui4Bc1.wav\n",
            " - src: \"hello and welcome back to how to bind\"\n",
            " - res: \"i \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.972973, loss: 128.188446\n",
            " - wav: file:///content/drive/My Drive/ytd_project/testing/transformed files/_-5_-10_1.1bankingLui4Bc1.wav\n",
            " - src: \"hello and welcome back to how to bind\"\n",
            " - res: \"i \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.972973, loss: 128.188446\n",
            " - wav: file:///content/drive/My Drive/ytd_project/testing/transformed files/_-5_5_1.1bankingLui4Bc1.wav\n",
            " - src: \"hello and welcome back to how to bind\"\n",
            " - res: \"i \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.972973, loss: 128.188446\n",
            " - wav: file:///content/drive/My Drive/ytd_project/testing/transformed files/_0_-10_1.1bankingLui4Bc1.wav\n",
            " - src: \"hello and welcome back to how to bind\"\n",
            " - res: \"i \"\n",
            "--------------------------------------------------------------------------------\n",
            "I Exporting the model...\n",
            "I Loading best validating checkpoint from /content/drive/My Drive/transfer_learning/deepspeech-0.7.0-checkpoint/best_dev-747750\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "I Models exported at /content/drive/My Drive/transfer_learning/exported_model\n",
            "I Model metadata file saved to /content/drive/My Drive/transfer_learning/exported_model/author_model_0.0.1.md. Before submitting the exported model for publishing make sure all information in the metadata file is correct, and complete the URL fields.\n",
            "Process ForkPoolWorker-4:\n",
            "Process ForkPoolWorker-3:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}